[![CI](https://github.com/colinobrien22/monte-carlo-blackjack/actions/workflows/ci.yml/badge.svg)](https://github.com/colinobrien22/monte-carlo-blackjack/actions/workflows/ci.yml)

# monte-carlo-blackjack
Reinforcement learning project using Monte Carlo control to learn optimal Blackjack strategy.

# üÉè Monte Carlo Blackjack ‚Äî On-Policy MC Control

**One-liner:** A from-scratch implementation of **on-policy Monte Carlo control (Œµ-greedy)** that learns a hit/stand policy for Blackjack and beats random play, approaching basic strategy under matched rules.

---

## üîç Problem & Rules

**Goal:** Learn an action policy œÄ(s) for **hit/stand** that maximizes expected return.

**Rules used (for reproducibility):**
- 6-deck shoe; reshuffle when penetration threshold is reached (e.g., 75%)
- Dealer **stands on soft 17**
- No splits, no doubles (focus purely on hit/stand)
- (If implemented) Natural blackjack pays 3:2

**State representation (classic):**
- `player_sum` ‚àà [4..21]
- `dealer_upcard` ‚àà {A,2,‚Ä¶,10}
- `usable_ace` ‚àà {True, False}

**Actions:** `hit` or `stand`

---

## üß† Method (MC Control)

- **On-policy Monte Carlo** with **Œµ-greedy** exploration  
- Estimate **Q(s,a)** from episodic returns (first-visit or every-visit)  
- Policy improvement via **greedy(Q)** as Œµ decays

**Baselines:** Random policy and a rules-matched Basic Strategy (no splits/doubles).

---

## üöÄ How to Run (Menu-driven)

Run the script and pick an option from the menu:

python monte_carlo_blackjack.py

You will see:
================= Monte Carlo Blackjack =================
1) Play interactively
2) Train (then save Q to q_table.pkl)
3) Evaluate from saved Q (q_table.pkl)
4) Show policy table from saved Q
5) Train + Evaluate
0) Exit
========================================================
Select an option: 

What each option does

1) Play interactively

Human vs dealer. Handy to sanity-check rules (hit/stand logic, soft 17, reshuffle, etc.).

No Q-table needed.

2) Train (save Q to q_table.pkl)

Runs on-policy Monte Carlo control with Œµ-greedy exploration.

At the end, saves q_table.pkl (pickle of your Q[(state, action)] or equivalent).

Recommended to also write metrics/plots to ./results/ (learning curve, returns, etc.).

3) Evaluate from saved Q (q_table.pkl)

Loads the saved Q-table and runs N evaluation hands (e.g., 100k).

Prints win / push / loss rates and (optionally) saves a JSON/CSV to ./results/.

4) Show policy table from saved Q

Loads q_table.pkl, computes greedy policy, and prints/plots policy tables:

Hard totals vs dealer upcard

Soft totals (usable ace) vs dealer upcard

Optionally outputs PNGs to ./results/:

policy_heatmap_usable_ace.png

policy_heatmap_no_ace.png

5) Train + Evaluate

Runs training ‚Üí saves q_table.pkl ‚Üí immediately evaluates on a fixed eval set.

Great for quick experiments you can screenshot for the README / LinkedIn.

Default paths:

Q-table file: q_table.pkl at the project root (or ./results/q_table.pkl).

Figures/metrics: ./results/ (create this folder if it doesn‚Äôt exist).

## üìà Results



- **Learning improves:** MC win-rate rose from ~`X%` to `Y%` after `N` episodes.
- **Head-to-head (100k hands):**

| Policy | Win % | Push % | Loss % | Notes |
|--------|-------:|--------:|--------:|-------|
| MC     |   Y    |   P     |   L     | Œµ-greedy, first/every-visit |
| Basic  |   B    |  PB     |  LB     | rules-matched |
| Random |   R    |  PR     |  LR     | baseline |

### üìà Learning Curve

![Learning Curve](results/figures/learning_curve.png)

### ‚ô†Ô∏è Policy Heatmaps

**With Usable Ace:**
![Policy (Usable Ace)](results/figures/policy_heatmap_usable_ace.png)

**Without Usable Ace:**
![Policy (No Ace)](results/figures/policy_heatmap_no_ace.png)

*(Placeholder images will be replaced automatically after first training run ‚Äî saved to `/results/figures/`.)*

