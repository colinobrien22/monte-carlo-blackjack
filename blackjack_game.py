# -*- coding: utf-8 -*-
"""Blackjack_game.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N-zyswPlJkmBoMJ9Xk5grwHZGCaPuE6D
"""

import random
import pickle
import os
import numpy as np
import matplotlib.pyplot as plt

# -----------------------
# Card + Hand Helpers
# -----------------------
def rank_to_str(rank):
    if rank == 1:
        return "A"
    elif rank == 11:
        return "J"
    elif rank == 12:
        return "Q"
    elif rank == 13:
        return "K"
    else:
        return str(rank)

def hand_to_str(hand):
    return ", ".join([rank_to_str(card) for card in hand])

def create_shoe(decks=6):
    single_deck = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] * 4
    shoe = single_deck * decks
    random.shuffle(shoe)
    return shoe

def deal_card(shoe):
    if len(shoe) == 0:
        shoe[:] = create_shoe()
    return shoe.pop()

def card_value(card):
    if card in [11, 12, 13]:
        return 10
    elif card == 1:
        return 1
    else:
        return card

def hand_value(hand):
    value = 0
    ace_count = 0
    is_soft = False
    for card in hand:
        value += card_value(card)
        if card == 1:
            ace_count += 1
    while ace_count > 0 and value + 10 <= 21:
        value += 10
        is_soft = True
        ace_count -= 1
    return value, is_soft

def dealer_play(hand, shoe, hit_on_soft_17=True):
    hand = hand[:]
    total, is_soft = hand_value(hand)
    while (total < 17) or (total == 17 and is_soft and hit_on_soft_17):
        hand.append(deal_card(shoe))
        total, is_soft = hand_value(hand)
    return hand

def is_blackjack(hand):
    total, _ = hand_value(hand)
    return len(hand) == 2 and total == 21

def resolve_outcome(player_hand, dealer_hand, count_blackjack=True):
    p_total, _ = hand_value(player_hand)
    d_total, _ = hand_value(dealer_hand)

    if count_blackjack:
        p_bj = is_blackjack(player_hand)
        d_bj = is_blackjack(dealer_hand)
        if p_bj and d_bj:
            return "push"
        elif p_bj:
            return "blackjack player wins"
        elif d_bj:
            return "blackjack dealer wins"

    if p_total > 21:
        return "busts player lost"
    elif d_total > 21:
        return "dealer busts player wins"
    elif p_total == d_total:
        return "push"
    elif p_total > d_total:
        return "player wins"
    else:
        return "dealer wins"

# -----------------------
# Interactive play_round
# -----------------------
def play_round(shoe, player_policy, hit_on_soft_17=True):
    player_hand = [deal_card(shoe), deal_card(shoe)]
    dealer_hand = [deal_card(shoe), deal_card(shoe)]
    dealer_upcard = dealer_hand[0]

    p_bj = is_blackjack(player_hand)
    d_bj = is_blackjack(dealer_hand)
    if p_bj and d_bj: return "push", player_hand, dealer_hand
    if p_bj: return "blackjack player wins", player_hand, dealer_hand
    if d_bj: return "blackjack dealer wins", player_hand, dealer_hand

    while True:
        action = player_policy(player_hand, dealer_upcard)  # "hit"/"stand"
        if action == "hit":
            player_hand.append(deal_card(shoe))
            total, _ = hand_value(player_hand)
            if total > 21:
                return "busts player lost", player_hand, dealer_hand
        elif action == "stand":
            break

    dealer_hand = dealer_play(dealer_hand, shoe, hit_on_soft_17=hit_on_soft_17)
    result = resolve_outcome(player_hand, dealer_hand, count_blackjack=True)
    return result, player_hand, dealer_hand

def interactive_player_policy(player_hand, dealer_upcard):
    total, is_soft = hand_value(player_hand)
    print(f"Your hand: {hand_to_str(player_hand)} (total {total}, {'soft' if is_soft else 'hard'})")
    print(f"Dealer upcard: {rank_to_str(dealer_upcard)}")
    while True:
        choice = input("[H]it or [S]tand? ").strip().lower()
        if choice in ("h","hit"): return "hit"
        if choice in ("s","stand"): return "stand"
        print("Please enter H or S.")

# -----------------------
# Policies
# -----------------------
def state_from(player_hand, dealer_upcard):
    player_total, is_soft = hand_value(player_hand)
    dealer_value = card_value(dealer_upcard)
    return (player_total, dealer_value, is_soft)

def choose_action_epsilon_greedy(Q, state, epsilon):
    if state not in Q:
        Q[state] = [0.0, 0.0]  # [hit, stand]
    if random.random() < epsilon:
        return random.choice([0, 1])  # explore
    elif Q[state][0] > Q[state][1]:
        return 0  # hit
    elif Q[state][1] > Q[state][0]:
        return 1  # stand
    else:
        return 1  # tie → default to stand

def greedy_action(Q, state):
    if state not in Q:
        return 1
    elif Q[state][0] > Q[state][1]:
        return 0
    elif Q[state][1] > Q[state][0]:
        return 1
    else:
        return 1

# -----------------------
# Training
# -----------------------
def generate_episode(Q, epsilon, shoe, hit_on_soft_17=True, count_blackjack=True):
    player_hand = [deal_card(shoe), deal_card(shoe)]
    dealer_hand = [deal_card(shoe), deal_card(shoe)]
    dealer_upcard = dealer_hand[0]

    if count_blackjack:
        p_bj = is_blackjack(player_hand)
        d_bj = is_blackjack(dealer_hand)
        if p_bj and d_bj: return [], 0
        if p_bj: return [], +1
        if d_bj: return [], -1

    episode = []
    final_reward = None
    while True:
        state = state_from(player_hand, dealer_upcard)
        action = choose_action_epsilon_greedy(Q, state, epsilon)  # 0=hit, 1=stand
        episode.append((state, action))

        if action == 0:
            player_hand.append(deal_card(shoe))
            total, _ = hand_value(player_hand)
            if total > 21:
                final_reward = -1
                break
        else:
            break

    if final_reward is None:
        dealer_hand = dealer_play(dealer_hand, shoe, hit_on_soft_17=hit_on_soft_17)
        outcome = resolve_outcome(player_hand, dealer_hand, count_blackjack=True)
        if ("wins" in outcome) and ("player" in outcome):
            final_reward = +1
        elif outcome == "push":
            final_reward = 0
        else:
            final_reward = -1

    return episode, final_reward

def update_Q(Q, N, episode, final_reward):
    visited = set()
    for state, action in episode:
        if (state, action) in visited:
            continue
        visited.add((state, action))

        if state not in Q:
            Q[state] = [0.0, 0.0]
        if (state, action) not in N:
            N[(state, action)] = 0

        N[(state, action)] += 1
        old_value = Q[state][action]
        n = N[(state, action)]
        new_value = old_value + (final_reward - old_value) / n
        Q[state][action] = new_value

# -----------------------
# Evaluation
# -----------------------
def play_greedy_hand(Q, shoe, hit_on_soft_17=True, count_blackjack=True):
    player_hand = [deal_card(shoe), deal_card(shoe)]
    dealer_hand = [deal_card(shoe), deal_card(shoe)]
    dealer_upcard = dealer_hand[0]

    if count_blackjack:
        p_bj = is_blackjack(player_hand)
        d_bj = is_blackjack(dealer_hand)
        if p_bj and d_bj:
            return 0
        elif p_bj:
            return +1.5
        elif d_bj:
            return -1

    while True:
        state = state_from(player_hand, dealer_upcard)
        action = greedy_action(Q, state)

        if action == 0:  # hit
            player_hand.append(deal_card(shoe))
            total, _ = hand_value(player_hand)
            if total > 21:
                return -1
        else:  # stand
            break

    dealer_hand = dealer_play(dealer_hand, shoe, hit_on_soft_17=hit_on_soft_17)
    outcome = resolve_outcome(player_hand, dealer_hand, count_blackjack=True)

    if "wins" in outcome and "player" in outcome:
        return +1
    if outcome == "push":
        return 0
    else:
        return -1

# -----------------------
# Wrappers
# -----------------------
def run_training(num_episodes=100_000, cut_card=52, eval_every=10000, eval_hands=5000):
    Q, N = {}, {}
    epsilon_start, epsilon_min, decay_rate = 0.2, 0.01, 0.99995
    epsilon = epsilon_start
    shoe = create_shoe()

    episodes_logged = []
    win_rates = []

    for i in range(1, num_episodes + 1):
        if len(shoe) < cut_card:
            shoe[:] = create_shoe()

        episode, final_reward = generate_episode(Q, epsilon, shoe)
        update_Q(Q, N, episode, final_reward)

        epsilon = max(epsilon_min, epsilon * decay_rate)

        # periodic checkpoint + optional monitor
        if i % 10000 == 0:
            sample_state = (11, 10, False)
            print(f"Episode {i}, epsilon={epsilon:.4f}, Q[{sample_state}]={Q.get(sample_state, [0.0,0.0])}")
            save_Q(Q, path="q_table.pkl")

        # log a quick win-rate for the learning curve
        if i % eval_every == 0:
            wr = evaluate_win_rate(Q, hands=eval_hands, cut_card=cut_card)
            episodes_logged.append(i)
            win_rates.append(wr)
            print(f"[monitor] after {i} episodes: win-rate ≈ {wr:.3f}")

    # final save
    save_Q(Q, "q_table.pkl")

    # save visuals
    save_learning_curve(episodes_logged, win_rates)
    save_policy_heatmaps(Q)

    return Q


def run_evaluation(Q, num_hands=100_000, cut_card=52, hit_on_soft_17=True, count_blackjack=True):
    shoe = create_shoe()
    wins = pushes = losses = 0
    net_units = 0.0
    for _ in range(num_hands):
        if len(shoe) < cut_card:
            shoe[:] = create_shoe()
        r = play_greedy_hand(Q, shoe, hit_on_soft_17=hit_on_soft_17, count_blackjack=count_blackjack)
        net_units += r

        if r > 0:
          wins += 1
        elif r == 0:
          pushes += 1
        else:
          losses += 1

    ev_per_hand = net_units / num_hands
    print(f"EV/hand: {ev_per_hand:.4f}")
    print(f"Hands: {num_hands}")
    print(f"Wins: {wins}  Pushes: {pushes}  Losses: {losses}")
    print(f"Win rate:  {wins/num_hands:.3f}")
    print(f"Push rate: {pushes/num_hands:.3f}")
    print(f"Loss rate: {losses/num_hands:.3f}")

def run_interactive():
    shoe = create_shoe()
    cut_card = 52
    while True:
        if len(shoe) < cut_card:
            print(f"\nShoe size: {len(shoe)} — reshuffling")
            shoe[:] = create_shoe()
        result, p_hand, d_hand = play_round(shoe, interactive_player_policy, hit_on_soft_17=True)
        print("\n--- Round Over ---")
        print("Player:", hand_to_str(p_hand), "=", hand_value(p_hand)[0])
        print("Dealer:", hand_to_str(d_hand), "=", hand_value(d_hand)[0])
        print("Result:", result)
        again = input("\nPlay again? (Y/N): ").strip().lower()
        if again not in ("y","yes"):
            print("Thanks for playing!")
            break

def save_Q(Q, path="q_table.pkl"):
    with open(path, "wb") as f:
        pickle.dump(Q, f)

def load_Q(path="q_table.pkl"):
    if not os.path.exists(path):
        print(f"[load_Q] No file at {path}. Returning empty Q.")
        return {}
    with open(path, "rb") as f:
        return pickle.load(f)
def run_evaluation_from_file(path="q_table.pkl", num_hands=100_000, cut_card=52,
                             hit_on_soft_17=True, count_blackjack=True):
    Q = load_Q(path)
    if not Q:
        print("Q is empty—did you train and save first?")
        return
    run_evaluation(Q, num_hands=num_hands, cut_card=cut_card,
                   hit_on_soft_17=hit_on_soft_17, count_blackjack=count_blackjack)

def quick_evaluate_greedy(Q, hands=1000, cut_card=52, hit_on_soft_17=True, count_blackjack=True):
    shoe = create_shoe()
    net_units = 0.0
    for _ in range(hands):
        if len(shoe) < cut_card:
            shoe[:] = create_shoe()
        r = play_greedy_hand(Q, shoe, hit_on_soft_17=hit_on_soft_17, count_blackjack=count_blackjack)
        net_units += r
    return net_units / hands  # EV per hand

def action_to_char(a):
  return "H" if a == 0 else "S"

def dealer_label(v):
  return "A" if v == 11 else ("T" if v == 10 else str(v))

def print_policy(Q):
    dealer_vals = list(range(2, 12))  # 2..11 (11 = Ace)

    # ---- Hard totals table ----
    print("\n=== HARD TOTALS (H/S) ===")
    header = "Tot | " + " ".join(f"{dealer_label(v):>2}" for v in dealer_vals)
    print(header)
    print("-" * len(header))

    for total in range(5, 21):  # 5..20
        row = [f"{total:>3} |"]
        for dv in dealer_vals:
            state = (total, dv, False)
            a = greedy_action(Q, state)
            row.append(f"{action_to_char(a):>2}")
        print(" ".join(row))

    # ---- Soft totals table ----
    print("\n=== SOFT TOTALS (H/S) ===")
    header = "Tot | " + " ".join(f"{dealer_label(v):>2}" for v in dealer_vals)
    print(header)
    print("-" * len(header))

    # soft totals A2..A9 correspond to 13..20
    for total in range(13, 21):  # 13..20
        row = [f"{total:>3} |"]
        for dv in dealer_vals:
            state = (total, dv, True)
            a = greedy_action(Q, state)
            row.append(f"{action_to_char(a):>2}")
        print(" ".join(row))

def evaluate_win_rate(Q, hands=5000, cut_card=52, hit_on_soft_17=True, count_blackjack=True):
     # Quick win-rate (fraction of wins) for the current greedy policy.
    shoe = create_shoe()
    wins = 0
    for _ in range(hands):
        if len(shoe) < cut_card:
            shoe[:] = create_shoe()
        r = play_greedy_hand(Q, shoe, hit_on_soft_17=hit_on_soft_17, count_blackjack=count_blackjack)
        if r > 0:
            wins += 1
    return wins / hands

def compute_policy_arrays(Q):
    # Build policy tables for heatmaps (0=Hit, 1=Stand).
    dealer_vals = list(range(2, 12))         # 2..11 (11=A)
    hard_totals = list(range(5, 21))         # 5..20
    soft_totals = list(range(13, 21))        # 13..20 (A2..A9)

    hard = np.zeros((len(hard_totals), len(dealer_vals)), dtype=int)
    soft = np.zeros((len(soft_totals), len(dealer_vals)), dtype=int)

    for i, total in enumerate(hard_totals):
        for j, dv in enumerate(dealer_vals):
            hard[i, j] = greedy_action(Q, (total, dv, False))

    for i, total in enumerate(soft_totals):
        for j, dv in enumerate(dealer_vals):
            soft[i, j] = greedy_action(Q, (total, dv, True))

    return (hard_totals, dealer_vals, hard), (soft_totals, dealer_vals, soft)

def plot_policy_heatmap(table, totals, dealer_vals, title, outfile):
    os.makedirs("results/figures", exist_ok=True)
    plt.figure()
    plt.imshow(table, origin="lower", aspect="auto")
    plt.xticks(range(len(dealer_vals)), [dealer_label(v) for v in dealer_vals])
    plt.yticks(range(len(totals)), totals)
    plt.colorbar(label="Action (0=Hit, 1=Stand)")
    plt.xlabel("Dealer Upcard")
    plt.ylabel("Player Total")
    plt.title(title)
    plt.savefig(outfile, bbox_inches="tight")
    plt.close()

def save_policy_heatmaps(Q):
    # Compute & save both policy heatmaps to results/figures.
    (hard_totals, dealer_vals, hard), (soft_totals, dealer_vals2, soft) = compute_policy_arrays(Q)
    plot_policy_heatmap(
        hard, hard_totals, dealer_vals,
        "Policy Heatmap (No Usable Ace)",
        "results/figures/policy_heatmap_no_ace.png"
    )
    plot_policy_heatmap(
        soft, soft_totals, dealer_vals2,
        "Policy Heatmap (Usable Ace)",
        "results/figures/policy_heatmap_usable_ace.png"
    )

def save_learning_curve(episodes, win_rates):
    # Save learning curve to results/figures/learning_curve.png.
    if not episodes or not win_rates:
        return
    os.makedirs("results/figures", exist_ok=True)
    plt.figure()
    plt.plot(episodes, win_rates, label="Win Rate")
    plt.xlabel("Episodes")
    plt.ylabel("Win Rate")
    plt.title("Monte Carlo Control Learning Curve")
    plt.legend()
    plt.savefig("results/figures/learning_curve.png", bbox_inches="tight")
    plt.close()


def main_menu():
    print("\n=== Blackjack RL Menu ===")
    print("1) Play interactively")
    print("2) Train (then save q_table.pkl)")
    print("3) Evaluate from saved Q (q_table.pkl)")
    print("4) Show policy table from saved Q")
    print("5) Train + Evaluate (one shot)")
    print("0) Exit")
    choice = input("Select: ").strip()
    return choice

# -----------------------
# Main Program
# -----------------------
if __name__ == "__main__":
    cut_card = 52  # default
    while True:
        choice = main_menu()

        if choice == "1":
            run_interactive()

        elif choice == "2":
            # ask for episodes (optional)
            try:
                eps = int(input("Num training episodes [100000]: ").strip() or "100000")
            except ValueError:
                eps = 100000
            Q = run_training(num_episodes=eps, cut_card=cut_card)
            print("Training complete. Saved to q_table.pkl")

        elif choice == "3":
            try:
                hands = int(input("Num evaluation hands [100000]: ").strip() or "100000")
            except ValueError:
                hands = 100000
            run_evaluation_from_file("q_table.pkl", num_hands=hands, cut_card=cut_card)

        elif choice == "4":
            Q = load_Q("q_table.pkl")
            if Q:
                print_policy(Q)
                save_policy_heatmaps(Q)
            else:
                print("No saved Q found. Choose 2) Train first.")

        elif choice == "5":
            # quick end-to-end run
            try:
                eps = int(input("Num training episodes [100000]: ").strip() or "100000")
            except ValueError:
                eps = 100000
            Q = run_training(num_episodes=eps, cut_card=cut_card)
            print_policy(Q)
            try:
                hands = int(input("Num evaluation hands [100000]: ").strip() or "100000")
            except ValueError:
                hands = 100000
            run_evaluation(Q, num_hands=hands, cut_card=cut_card)

        elif choice == "0":
            print("Bye!")
            break

        else:
            print("Invalid selection. Try again.")